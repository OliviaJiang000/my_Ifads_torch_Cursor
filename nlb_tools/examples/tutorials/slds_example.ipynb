{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://githubtocolab.com/neurallatents/nlb_tools/blob/main/examples/tutorials/slds_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLDS Demo\n",
    "\n",
    "In this notebook, we will use a switching linear dynamical system (SLDS) to model the neural data. We will use the Linderman Lab's [`ssm` package](https://github.com/lindermanlab/ssm), which you should install before running this demo. We recommend first viewing `basic_example.ipynb` for more explanation of the `nlb_tools` functions we use here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Below, we import the necessary functions from `nlb_tools` and additional standard packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install packages if necessary\n",
    "# !pip install git+https://github.com/lindermanlab/ssm\n",
    "# !pip install -U scikit-learn\n",
    "# !pip install nlb-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "from nlb_tools.nwb_interface import NWBDataset\n",
    "from nlb_tools.make_tensors import make_train_input_tensors, make_eval_input_tensors, make_eval_target_tensors, save_to_h5\n",
    "from nlb_tools.evaluation import evaluate\n",
    "\n",
    "import ssm\n",
    "import numpy as np\n",
    "import h5py\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If necessary, download dataset from DANDI\n",
    "# !pip install dandi\n",
    "# !dandi download https://dandiarchive.org/dandiset/000138 # replace URL with URL for dataset you want\n",
    "# # URLS are:\n",
    "# # - MC_Maze: https://dandiarchive.org/dandiset/000128\n",
    "# # - MC_RTT: https://dandiarchive.org/dandiset/000129\n",
    "# # - Area2_Bump: https://dandiarchive.org/dandiset/000127\n",
    "# # - DMFC_RSG: https://dandiarchive.org/dandiset/000130\n",
    "# # - MC_Maze_Large: https://dandiarchive.org/dandiset/000138\n",
    "# # - MC_Maze_Medium: https://dandiarchive.org/dandiset/000139\n",
    "# # - MC_Maze_Small: https://dandiarchive.org/dandiset/000140"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading data\n",
    "\n",
    "Below, please enter the path to the dataset, as well as the name of the dataset, to load the data. In addition, you can choose a bin size (0.005 or 0.02 s) to run the notebook at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load dataset\n",
    "\n",
    "dataset_name = 'mc_maze_small'\n",
    "datapath = './000140/sub-Jenkins/'\n",
    "prefix = f'*ses-small'\n",
    "dataset = NWBDataset(datapath, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Input prep\n",
    "\n",
    "`ssm` expects inputs as a list of 2d arrays of type `int`, so we will use functions from `make_tensors` to create 3d arrays, and split the arrays along the trial axis to get our list. Note that since SLDS can perform forward prediction, we indicate `include_forward_pred=True` in `make_train_input_tensors`, which includes the next 200 ms of spiking activity after the required window for each trial in separate tensors called `'train_spikes_heldin_forward'` and `'train_spikes_heldout_forward'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset preparation\n",
    "\n",
    "# Choose the phase here, either 'val' or 'test'\n",
    "phase = 'val'\n",
    "\n",
    "# Choose bin width and resample\n",
    "bin_width = 5\n",
    "dataset.resample(bin_width)\n",
    "\n",
    "# Create suffix for group naming later\n",
    "suffix = '' if (bin_width == 5) else f'_{int(round(bin_width))}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make train input data\n",
    "\n",
    "# Generate input tensors\n",
    "train_trial_split = 'train' if (phase == 'val') else ['train', 'val']\n",
    "train_dict = make_train_input_tensors(dataset, dataset_name=dataset_name, trial_split=train_trial_split, save_file=False, include_forward_pred=True)\n",
    "\n",
    "# Unpack input data\n",
    "train_spikes_heldin = train_dict['train_spikes_heldin']\n",
    "train_spikes_heldout = train_dict['train_spikes_heldout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make eval input data\n",
    "\n",
    "# Generate input tensors\n",
    "eval_trial_split = phase\n",
    "eval_dict = make_eval_input_tensors(dataset, dataset_name=dataset_name, trial_split=eval_trial_split, save_file=False)\n",
    "\n",
    "# Unpack data\n",
    "eval_spikes_heldin = eval_dict['eval_spikes_heldin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prep input\n",
    "\n",
    "# Combine train spiking data into one array\n",
    "train_spikes_heldin = train_dict['train_spikes_heldin']\n",
    "train_spikes_heldout = train_dict['train_spikes_heldout']\n",
    "train_spikes_heldin_fp = train_dict['train_spikes_heldin_forward']\n",
    "train_spikes_heldout_fp = train_dict['train_spikes_heldout_forward']\n",
    "train_spikes = np.concatenate([\n",
    "    np.concatenate([train_spikes_heldin, train_spikes_heldin_fp], axis=1),\n",
    "    np.concatenate([train_spikes_heldout, train_spikes_heldout_fp], axis=1),\n",
    "], axis=2)\n",
    "\n",
    "# Fill missing test spiking data with zeros and make masks\n",
    "eval_spikes_heldin = eval_dict['eval_spikes_heldin']\n",
    "eval_spikes = np.full((eval_spikes_heldin.shape[0], train_spikes.shape[1], train_spikes.shape[2]), 0.0)\n",
    "masks = np.full((eval_spikes_heldin.shape[0], train_spikes.shape[1], train_spikes.shape[2]), False)\n",
    "eval_spikes[:, :eval_spikes_heldin.shape[1], :eval_spikes_heldin.shape[2]] = eval_spikes_heldin\n",
    "masks[:, :eval_spikes_heldin.shape[1], :eval_spikes_heldin.shape[2]] = True\n",
    "\n",
    "# Make lists of arrays\n",
    "train_datas = [train_spikes[i, :, :].astype(int) for i in range(len(train_spikes))]\n",
    "eval_datas = [eval_spikes[i, :, :].astype(int) for i in range(len(eval_spikes))]\n",
    "eval_masks = [masks[i, :, :].astype(bool) for i in range(len(masks))]\n",
    "\n",
    "num_heldin = train_spikes_heldin.shape[2]\n",
    "tlen = train_spikes_heldin.shape[1]\n",
    "num_train = len(train_datas)\n",
    "num_eval = len(eval_datas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running SLDS\n",
    "\n",
    "Now that we have our input data prepared, we can fit an SLDS to it. Feel free to vary the parameters as you see fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f56b9137d754192821a85315bf33de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing with an ARHMM using 25 steps of EM.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99254f1a302c4fb9a37595dbd4bde07d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8a30354b2449ce9aa374a62f466542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e212c9b881d746fd9899a991536064d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Run SLDS\n",
    "\n",
    "# Set parameters\n",
    "T = train_datas[0].shape[0] # trial length\n",
    "K = 3 # number of discrete states\n",
    "D = 15 # dimensionality of latent states\n",
    "N = train_datas[0].shape[1] # input dimensionality\n",
    "\n",
    "slds = ssm.SLDS(N, K, D,\n",
    "    transitions='standard',\n",
    "    emissions='poisson',\n",
    "    emission_kwargs=dict(link=\"log\"),\n",
    "    dynamics_kwargs={\n",
    "        'l2_penalty_A': 3000.0,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Train\n",
    "q_elbos_lem_train, q_lem_train = slds.fit(\n",
    "    datas=train_datas,\n",
    "    method=\"laplace_em\",\n",
    "    variational_posterior=\"structured_meanfield\",\n",
    "    num_init_iters=25, num_iters=25, alpha=0.2,\n",
    ")\n",
    "\n",
    "# Pass eval data\n",
    "q_elbos_lem_eval, q_lem_eval = slds.approximate_posterior(\n",
    "    datas=eval_datas,\n",
    "    masks=eval_masks,\n",
    "    method=\"laplace_em\",\n",
    "    variational_posterior=\"structured_meanfield\",\n",
    "    num_iters=25, alpha=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generating rate predictions\n",
    "\n",
    "We now have our estimates of continuous neural population state, so we'll now use them to predict neuron firing rates. `SLDS` does this by smoothing the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate rate predictions\n",
    "\n",
    "# Smooth observations using inferred states\n",
    "train_rates = [slds.smooth(q_lem_train.mean_continuous_states[i], train_datas[i]) for i in range(num_train)]\n",
    "eval_rates = [slds.smooth(q_lem_eval.mean_continuous_states[i], eval_datas[i], mask=eval_masks[i]) for i in range(num_eval)]\n",
    "\n",
    "# Reshape output\n",
    "train_rates = np.stack(train_rates)\n",
    "eval_rates = np.stack(eval_rates)\n",
    "\n",
    "train_rates_heldin = train_rates[:, :tlen, :num_heldin]\n",
    "train_rates_heldout = train_rates[:, :tlen, num_heldin:]\n",
    "eval_rates_heldin = eval_rates[:, :tlen, :num_heldin]\n",
    "eval_rates_heldout = eval_rates[:, :tlen, num_heldin:]\n",
    "eval_rates_heldin_forward = eval_rates[:, tlen:, :num_heldin]\n",
    "eval_rates_heldout_forward = eval_rates[:, tlen:, num_heldin:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Making the submission\n",
    "\n",
    "Now, we'll make the submission dict manually. As described in `basic_example.ipynb`, you can also use the function `save_to_h5` from `make_tensors.py` to save the output as an h5 file for submission on EvalAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare submission data\n",
    "\n",
    "output_dict = {\n",
    "    dataset_name + suffix: {\n",
    "        'train_rates_heldin': train_rates_heldin,\n",
    "        'train_rates_heldout': train_rates_heldout,\n",
    "        'eval_rates_heldin': eval_rates_heldin,\n",
    "        'eval_rates_heldout': eval_rates_heldout,\n",
    "        'eval_rates_heldin_forward': eval_rates_heldin_forward,\n",
    "        'eval_rates_heldout_forward': eval_rates_heldout_forward,\n",
    "    }\n",
    "}\n",
    "\n",
    "# To save as an h5 file:\n",
    "# save_to_h5(output_dict, 'submission.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation\n",
    "\n",
    "Finally, we will create the test data with make_test_tensor and evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zero rate predictions found. Replacing zeros with 1e-9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'mc_maze_scaling_split': {'[100] co-bps': 0.15331211388512617, '[100] vel R2': 0.6514445468827901, '[100] psth R2': 0.21864536599656217, '[100] fp-bps': -2.7198655704068924}}]\n"
     ]
    }
   ],
   "source": [
    "## Make data to test predictions with and evaluate\n",
    "\n",
    "if phase == 'val':\n",
    "    target_dict = make_eval_target_tensors(dataset, dataset_name=dataset_name, train_trial_split='train', eval_trial_split='val', include_psth=('mc_rtt' not in dataset_name), save_file=False)\n",
    "\n",
    "    print(evaluate(target_dict, output_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we used `nlb_tools` and `ssm` to run and evaluate SLDS on our benchmark."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d876f2d84ebe613ecb987c3cdf86da35455b4fa2dba2ba72805210f4933655ff"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('tf2-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "d876f2d84ebe613ecb987c3cdf86da35455b4fa2dba2ba72805210f4933655ff"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
